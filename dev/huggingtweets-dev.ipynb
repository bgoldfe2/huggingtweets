{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXeYEzBG7g0H"
   },
   "source": [
    "# HuggingTweets - Tweet Generation with Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZuE848q7g0I"
   },
   "source": [
    "*Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation (NLG).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "668-beiK7g0J"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZipizzvM7g0J"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Generating realistic text has become more and more efficient with models such as [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf). Those models are trained on very large datasets and require heavy computer resources (and time!).\n",
    "\n",
    "However, we can use Transfer Learning and a single GPU to quickly fine-tune a pre-trained model on a given task.\n",
    "\n",
    "We test if we can imitate the writing style of a Twitter user by only using some of his tweets. Twitter API let us download \"only\" the 3200 most recent tweets from any single user, which we then filter out (to remove retweets, short content, etc).\n",
    "\n",
    "[HuggingFace](https://huggingface.co/) gives us an easy access to pre-trained models and fine-tuning techniques for Natural Language Generation (NLG) tasks.\n",
    "\n",
    "We will be monitoring the training with [W&B](https://docs.wandb.com/huggingface) (which is integrated in HuggingFace) to ensure the model is learning from the data and compare multiple experiments.\n",
    "\n",
    "![](https://i.imgur.com/vnejHGh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wODF0Bx27g0K"
   },
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxRDBrFi7g0L"
   },
   "outputs": [],
   "source": [
    "# install required libraries are not installed\n",
    "# These are installed locally on the 3900 system using conda environment 'huggingface' and a pip -r requirements.txt command line run\n",
    "!pip install torch -qq\n",
    "!pip install transformers -qq\n",
    "!pip install wandb -qq\n",
    "!pip install tweepy -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4IRQjZw37g0P"
   },
   "outputs": [],
   "source": [
    "# HuggingFace scripts for fine-tuning models and language generation\n",
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py -q\n",
    "#!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/text-generation/run_generation.py -q\n",
    "#Note: There is a newer library for running language modeling at https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
    "#         The new libaries break out the different base training models based on type of training\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/legacy/run_language_modeling.py -q\n",
    "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-generation/run_generation.py -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guKG3bqa7g0S"
   },
   "source": [
    "## Set up a Twitter Development Account\n",
    "\n",
    "In order to access Twitter data, we need to:\n",
    "\n",
    "* [create a Twitter development account](https://developer.twitter.com/en/apply-for-access)\n",
    "* [create a Twitter app](https://developer.twitter.com/en/apps)\n",
    "* get your consumer API keys: \"API key\" and \"API secret key\"\n",
    "\n",
    "The entire process only takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NI1ktZNM7g0S"
   },
   "outputs": [],
   "source": [
    "# <--- Enter your credentials (don't share with anyone) --->\n",
    "consumer_key = 'Lj1RBpWSAVVSp5isWyC5BTcHV'\n",
    "consumer_secret = 'OIJsiMBeOinsakM1Ech37ARp5fIxMGbtBhAGLSm0jZZMidY2hw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqe4FwQz7g0V"
   },
   "source": [
    "## Download tweets from a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yx3QyAXc7g0V"
   },
   "source": [
    "We download latest tweets associated to a user account through [Tweepy](http://docs.tweepy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4SMOIzp7g0W"
   },
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sReCnnPC7g0Y"
   },
   "outputs": [],
   "source": [
    "# authenticate\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXwG7box7g0b"
   },
   "source": [
    "We grab all available tweets (limited to 3200 per API limitations) based on Twitter handle.\n",
    "\n",
    "**Note**: Protected users may only be requested when the authenticated user either \"owns\" the timeline or is an approved follower of the owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOkUI-XL7g0c"
   },
   "outputs": [],
   "source": [
    "# <--- Enter the screen name of the user you will download your dataset from --->\n",
    "handle = 'elonmusk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYTl-WMU7g0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... 50 tweets downloaded so far\n",
      "Grabbed 50 tweets\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://gist.github.com/onmyeoin/62c72a7d61fc840b2689b2cf106f583c\n",
    "\n",
    "# initialize a list to hold all the tweepy Tweets & list with no retweets\n",
    "alltweets = []\n",
    "\n",
    "# make initial request for most recent tweets with extended mode enabled to get full tweets\n",
    "new_tweets = api.user_timeline(\n",
    "    screen_name=handle, tweet_mode='extended', count=200)\n",
    "\n",
    "if new_tweets:\n",
    "    # save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    # save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    # keep grabbing tweets until the api limit is reached\n",
    "    while True:\n",
    "        # all subsequent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(\n",
    "            screen_name=handle, tweet_mode='extended', count=200, max_id=oldest)\n",
    "        \n",
    "        # stop if no more tweets (try a few times as they sometimes eventually come)\n",
    "        if not new_tweets:\n",
    "            break\n",
    "\n",
    "        # save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        # update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        print(f'... {len(alltweets)} tweets downloaded so far')\n",
    "\n",
    "n_tweets = len(alltweets)\n",
    "print(f'Grabbed {n_tweets} tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0F7IRQWVAQqm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 tweets, including 9 RT, keeping 41\n"
     ]
    }
   ],
   "source": [
    "# get text and remove RT\n",
    "my_tweets = [tweet.full_text for tweet in alltweets if not hasattr(tweet, 'retweeted_status')]\n",
    "print(f'Found {n_tweets} tweets, including {n_tweets - len(my_tweets)} RT, keeping {len(my_tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_FXOXqO7g0i"
   },
   "source": [
    "## Create a dataset from downloaded tweets\n",
    "\n",
    "We remove:\n",
    "* retweets (since it's not in the wording style of target author)\n",
    "* tweets with no interesting content (limited to url's, user mentionss, \"thank you\"…)\n",
    "\n",
    "We clean up remaining tweets:\n",
    "* we remove url's\n",
    "* we correct special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlLlFGsS7g0i"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMGMTzkW7g0z"
   },
   "source": [
    "We verify our list of tweets is well curated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trct87MU7g0z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets: 50\n",
      "My tweets: 41\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of tweets: {len(alltweets)}\\nMy tweets: {len(my_tweets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VWYjAkFI7g02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweets\n",
      "\n",
      "RT @NASA: Four new astronauts through the hatch and seven crewmembers total on the @Space_Station!\n",
      "\n",
      "After almost exactly a day from launch,…\n",
      "\n",
      "@teslaownersSV @24_7TeslaNews @SpaceX No guarantees, but maybe next month. Requires quite a lot of incremental testing &amp; code tweaks for different road system in Canada.\n",
      "\n",
      "@24_7TeslaNews @teslaownersSV @SpaceX Hoping to start releasing to 98 scores with V10.5 in about 10 days\n",
      "\n",
      "@teslaownersSV @SpaceX Ancient times\n",
      "\n",
      "@SamTwits I hope they’re able to achieve high production &amp; breakeven cash flow. That is the true test. \n",
      "\n",
      "There have been hundreds of automotive startups, both electric &amp; combustion, but Tesla is only American carmaker to reach high volume production &amp; positive cash flow in past 100 years.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original tweets\\n')\n",
    "for t in alltweets[:5]:\n",
    "    print(f'{t.full_text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "999yftxF7g04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My tweets\n",
      "\n",
      "@teslaownersSV @24_7TeslaNews @SpaceX No guarantees, but maybe next month. Requires quite a lot of incremental testing &amp; code tweaks for different road system in Canada.\n",
      "\n",
      "@24_7TeslaNews @teslaownersSV @SpaceX Hoping to start releasing to 98 scores with V10.5 in about 10 days\n",
      "\n",
      "@teslaownersSV @SpaceX Ancient times\n",
      "\n",
      "@SamTwits I hope they’re able to achieve high production &amp; breakeven cash flow. That is the true test. \n",
      "\n",
      "There have been hundreds of automotive startups, both electric &amp; combustion, but Tesla is only American carmaker to reach high volume production &amp; positive cash flow in past 100 years.\n",
      "\n",
      "@PPathole @SpaceX Pattern on the Starlink router is orbital transfer ellipse from Earth to Mars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('My tweets\\n')\n",
    "for t in my_tweets[:5]:\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0a3kEpYv7g09"
   },
   "source": [
    "We remove boring tweets (tweets with only urls or too short) and cleanup texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vx-1pzGDMhp8"
   },
   "outputs": [],
   "source": [
    "def fix_text(text):\n",
    "    text = text.replace('&amp;', '&')\n",
    "    text = text.replace('&lt;', '<')\n",
    "    text = text.replace('&gt;', '>')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e0FXtb_sMkaC"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, allow_new_lines = False):\n",
    "    bad_start = ['http:', 'https:']\n",
    "    for w in bad_start:\n",
    "        tweet = re.sub(f\" {w}\\\\S+\", \"\", tweet)      # removes white space before url\n",
    "        tweet = re.sub(f\"{w}\\\\S+ \", \"\", tweet)      # in case a tweet starts with a url\n",
    "        tweet = re.sub(f\"\\n{w}\\\\S+ \", \"\", tweet)    # in case the url is on a new line\n",
    "        tweet = re.sub(f\"\\n{w}\\\\S+\", \"\", tweet)     # in case the url is alone on a new line\n",
    "        tweet = re.sub(f\"{w}\\\\S+\", \"\", tweet)       # any other case?\n",
    "    tweet = re.sub(' +', ' ', tweet)                # replace multiple spaces with one space (makes the previous work worthless?)\n",
    "    if not allow_new_lines:                         # TODO: predictions seem better without new lines\n",
    "        tweet = ' '.join(tweet.split())\n",
    "    return tweet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaSI1NZtMn3G"
   },
   "outputs": [],
   "source": [
    "def boring_tweet(tweet):\n",
    "    \"Check if this is a boring tweet\"\n",
    "    boring_stuff = ['http', '@', '#']\n",
    "    not_boring_words = len([None for w in tweet.split() if all(bs not in w.lower() for bs in boring_stuff)])\n",
    "    return not_boring_words < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw3L46xzNFGV"
   },
   "outputs": [],
   "source": [
    "curated_tweets = [fix_text(tweet) for tweet in my_tweets]\n",
    "clean_tweets = [clean_tweet(tweet) for tweet in curated_tweets]\n",
    "cool_tweets = [tweet for tweet in clean_tweets if not boring_tweet(tweet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3SrMQuZ47g1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated tweets: 41\n",
      "Cool tweets: 28\n"
     ]
    }
   ],
   "source": [
    "print(f'Curated tweets: {len(curated_tweets)}\\nCool tweets: {len(cool_tweets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkXwJxQl7g1D"
   },
   "source": [
    "We split data into training and validation sets (90/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dKVICXQ7g1E"
   },
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "random.shuffle(cool_tweets)\n",
    "\n",
    "# fraction of training data\n",
    "split_train_valid = 0.9\n",
    "\n",
    "# split dataset\n",
    "train_size = int(split_train_valid * len(cool_tweets))\n",
    "valid_size = len(cool_tweets) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(cool_tweets, [train_size, valid_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhjgM8As7g1G"
   },
   "source": [
    "We export our datasets as text files, simulating number of epochs by mixing up tweets, due to one batch containing multiple tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIutO2HNvuRg"
   },
   "outputs": [],
   "source": [
    "def make_dataset(dataset, epochs):\n",
    "    total_text = '<|endoftext|>'\n",
    "    tweets = [t for t in dataset]\n",
    "    for _ in range(epochs):\n",
    "        random.shuffle(tweets)\n",
    "        total_text += '<|endoftext|>'.join(tweets) + '<|endoftext|>'\n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OjOW_4x7g1H"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "with open('{}_train.txt'.format(handle), 'w') as f:\n",
    "    data = make_dataset(train_dataset, EPOCHS)\n",
    "    f.write(data)\n",
    "\n",
    "with open('{}_valid.txt'.format(handle), 'w') as f:\n",
    "    data = make_dataset(valid_dataset, 1)\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKfJrfl57g1J"
   },
   "source": [
    "## Log and monitor training through W&B\n",
    "\n",
    "In order to check our model is training correctly and compare experiments, we are going to use the W&B integration from HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cD3v1dDK7g1J"
   },
   "source": [
    "### API Key\n",
    "Once you've signed up, run the next cell and click on the link to get your API key and authenticate this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDu7g4dy7g1K"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgoldfe2\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeCgMAKo7g1N"
   },
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "HuggingFace includes the script `run_language_modeling` making it easy to fine-tune a pre-trained model.\n",
    "\n",
    "We use a pre-trained GPT-2 model and fine-tune it on our dataset.\n",
    "\n",
    "Training is automatically logged on W&B (see [documentation](https://docs.wandb.com/huggingface)). Urls are generated to visualize ongoing runs or you can just open your [dashboard](http://app.wandb.ai/).\n",
    "\n",
    "I quickly tested running for several epochs and my run was showing I started overfitting after 4 epochs so this is the limit I use to fine-tune my model (takes less than 2 minutes).\n",
    "\n",
    "![](https://i.imgur.com/1uIxLFe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fA0SYlIU7g1N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=my-test-project\n"
     ]
    }
   ],
   "source": [
    "# Associate run to a project (optional)\n",
    "#%env WANDB_PROJECT=huggingtweets-dev\n",
    "%env WANDB_PROJECT=my-test-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJQoCJV97g1Q"
   },
   "source": [
    "We use HuggingFace script `run_language_modeling.py` to fine-tune our model (see [doc](https://huggingface.co/transformers/)).\n",
    "\n",
    "*Note: epochs are built into the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/12/2021 12:09:58 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/12/2021 12:09:58 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=20,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/test-mlm/runs/Nov12_12-09-58_bruce-3090-PhD,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=/tmp/test-mlm,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/test-mlm,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/12/2021 12:09:58 - WARNING - datasets.builder - Using custom data configuration default-3433563c28928e21\n",
      "11/12/2021 12:09:58 - INFO - datasets.builder - Generating dataset text (/home/bruce/.cache/huggingface/datasets/text/default-3433563c28928e21/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Downloading and preparing dataset text/default to /home/bruce/.cache/huggingface/datasets/text/default-3433563c28928e21/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 4181.76it/s]\n",
      "11/12/2021 12:09:58 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "11/12/2021 12:09:58 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1180.50it/s]\n",
      "11/12/2021 12:09:58 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "11/12/2021 12:09:58 - INFO - datasets.builder - Generating split train\n",
      "11/12/2021 12:09:58 - INFO - datasets.builder - Generating split validation\n",
      "11/12/2021 12:09:58 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset text downloaded and prepared to /home/bruce/.cache/huggingface/datasets/text/default-3433563c28928e21/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1407.72it/s]\n",
      "[INFO|configuration_utils.py:588] 2021-11-12 12:09:58,885 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/bruce/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:625] 2021-11-12 12:09:58,887 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:343] 2021-11-12 12:09:58,931 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:588] 2021-11-12 12:09:58,976 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/bruce/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:625] 2021-11-12 12:09:58,978 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-12 12:09:59,320 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/bruce/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-12 12:09:59,320 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/bruce/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-12 12:09:59,321 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/bruce/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-12 12:09:59,321 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-12 12:09:59,321 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1742] 2021-11-12 12:09:59,321 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:588] 2021-11-12 12:09:59,375 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/bruce/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:625] 2021-11-12 12:09:59,377 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1342] 2021-11-12 12:09:59,471 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/bruce/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1609] 2021-11-12 12:10:00,301 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1617] 2021-11-12 12:10:00,301 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Running tokenizer on every text in dataset:   0%|         | 0/1 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3308] 2021-11-12 12:10:00,324 >> Token indices sequence length is longer than the specified maximum sequence length for this model (4025 > 512). Running this sequence through the model will result in indexing errors\n",
      "11/12/2021 12:10:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bruce/.cache/huggingface/datasets/text/default-3433563c28928e21/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-39b738fa776bc116.arrow\n",
      "Running tokenizer on every text in dataset: 100%|█| 1/1 [00:00<00:00, 164.06ba/s\n",
      "Running tokenizer on every text in dataset:   0%|         | 0/1 [00:00<?, ?ba/s]11/12/2021 12:10:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bruce/.cache/huggingface/datasets/text/default-3433563c28928e21/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-509157548115f442.arrow\n",
      "Running tokenizer on every text in dataset: 100%|█| 1/1 [00:00<00:00, 1083.52ba/\n",
      "Grouping texts in chunks of 512:   0%|                    | 0/1 [00:00<?, ?ba/s]11/12/2021 12:10:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bruce/.cache/huggingface/datasets/text/default-3433563c28928e21/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-e6dd98bac7bdfaa8.arrow\n",
      "Grouping texts in chunks of 512: 100%|███████████| 1/1 [00:00<00:00, 328.19ba/s]\n",
      "Grouping texts in chunks of 512:   0%|                    | 0/1 [00:00<?, ?ba/s]11/12/2021 12:10:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/bruce/.cache/huggingface/datasets/text/default-3433563c28928e21/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-e29f2b618a82d5fb.arrow\n",
      "Grouping texts in chunks of 512: 100%|██████████| 1/1 [00:00<00:00, 1682.43ba/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[INFO|trainer.py:540] 2021-11-12 12:10:02,346 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[WARNING|training_args.py:874] 2021-11-12 12:10:02,347 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:874] 2021-11-12 12:10:02,347 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1196] 2021-11-12 12:10:02,349 >> ***** Running training *****\n",
      "[INFO|trainer.py:1197] 2021-11-12 12:10:02,349 >>   Num examples = 7\n",
      "[INFO|trainer.py:1198] 2021-11-12 12:10:02,349 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1199] 2021-11-12 12:10:02,349 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1200] 2021-11-12 12:10:02,349 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1201] 2021-11-12 12:10:02,349 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1202] 2021-11-12 12:10:02,349 >>   Total optimization steps = 7\n",
      "[INFO|integrations.py:502] 2021-11-12 12:10:02,350 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "[WARNING|training_args.py:874] 2021-11-12 12:10:02,350 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbgoldfe2\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/tmp/test-mlm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/bgoldfe2/my-test-project\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/bgoldfe2/my-test-project/runs/3bhrbqka\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /home/bruce/dev/huggingtweets/dev/wandb/run-20211112_121002-3bhrbqka\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      " 86%|██████████████████████████████████████▌      | 6/7 [00:00<00:00, 13.79it/s][INFO|trainer.py:1409] 2021-11-12 12:10:05,084 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2.7369, 'train_samples_per_second': 2.558, 'train_steps_per_second': 2.558, 'train_loss': 1.8326988220214844, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 12.07it/s]\n",
      "[INFO|trainer.py:1995] 2021-11-12 12:10:05,087 >> Saving model checkpoint to /tmp/test-mlm\n",
      "[INFO|configuration_utils.py:417] 2021-11-12 12:10:05,088 >> Configuration saved in /tmp/test-mlm/config.json\n",
      "[INFO|modeling_utils.py:1060] 2021-11-12 12:10:05,643 >> Model weights saved in /tmp/test-mlm/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2037] 2021-11-12 12:10:05,644 >> tokenizer config file saved in /tmp/test-mlm/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2043] 2021-11-12 12:10:05,644 >> Special tokens file saved in /tmp/test-mlm/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.8327\n",
      "  train_runtime            = 0:00:02.73\n",
      "  train_samples            =          7\n",
      "  train_samples_per_second =      2.558\n",
      "  train_steps_per_second   =      2.558\n",
      "11/12/2021 12:10:05 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:540] 2021-11-12 12:10:05,695 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2243] 2021-11-12 12:10:05,696 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2245] 2021-11-12 12:10:05,696 >>   Num examples = 1\n",
      "[INFO|trainer.py:2248] 2021-11-12 12:10:05,696 >>   Batch size = 8\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]***** eval metrics *****\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 2086.72it/s]\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     3.9497\n",
      "  eval_runtime            = 0:00:00.00\n",
      "  eval_samples            =          1\n",
      "  eval_samples_per_second =    118.846\n",
      "  eval_steps_per_second   =    118.846\n",
      "  perplexity              =    51.9213\n",
      "[WARNING|training_args.py:874] 2021-11-12 12:10:05,705 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:874] 2021-11-12 12:10:05,705 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|modelcard.py:449] 2021-11-12 12:10:05,739 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}}\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 95559... (success).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch ▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step ▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss 3.94973\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime 0.0084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second 118.846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second 118.846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos 1842858243072.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss 1.8327\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime 2.7369\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second 2.558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second 2.558\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m/tmp/test-mlm\u001b[0m: \u001b[34mhttps://wandb.ai/bgoldfe2/my-test-project/runs/3bhrbqka\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211112_121002-3bhrbqka/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
     ]
    }
   ],
   "source": [
    "# Using the nightly build and the run_mlm.py script\n",
    "# removed the parameter --evaluate_during_training \\ due to error perhaps it was deprecated\n",
    "!python run_mlm.py \\\n",
    "    --output_dir /tmp/test-mlm \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache \\\n",
    "    --model_type roberta-base \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --do_train --train_file ./elonmusk_train.txt \\\n",
    "    --do_eval --validation_file ./elonmusk_valid.txt \\\n",
    "    --eval_steps 20 \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4LWV56z7g1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bruce/anaconda3/envs/huggingface/lib/python3.9/site-packages/torch/cuda/__init__.py:143: UserWarning: \n",
      "NVIDIA GeForce RTX 3090 with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3090 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bruce/dev/huggingtweets/dev/run_language_modeling.py\", line 364, in <module>\n",
      "    main()\n",
      "  File \"/home/bruce/dev/huggingtweets/dev/run_language_modeling.py\", line 195, in main\n",
      "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "  File \"/home/bruce/anaconda3/envs/huggingface/lib/python3.9/site-packages/transformers/hf_argparser.py\", line 215, in parse_args_into_dataclasses\n",
      "    raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {remaining_args}\")\n",
      "ValueError: Some specified arguments are not used by the HfArgumentParser: ['--evaluate_during_training']\n"
     ]
    }
   ],
   "source": [
    "# Legacy version used by original code\n",
    "!python run_language_modeling.py \\\n",
    "    --output_dir=output/$handle \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=$handle\\_train.txt \\\n",
    "    --do_eval --eval_data_file=$handle\\_valid.txt \\\n",
    "    --evaluate_during_training \\\n",
    "    --eval_steps 20 \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viSAJ7EE7g1T"
   },
   "source": [
    "## Let's test our trained model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffHmhI9P7g1T"
   },
   "source": [
    "We test our model on a few sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQRNMedK7g1T"
   },
   "outputs": [],
   "source": [
    "SENTENCES = [\"I think that\",\n",
    "             \"I like\",\n",
    "             \"I don't like\",\n",
    "             \"I want\",\n",
    "             \"My dream is\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ImkNghlH7g1V"
   },
   "source": [
    "We use HuggingFace script `run_generation.py` to generate sentences (see [doc](https://huggingface.co/transformers/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dK-R3bPP7g1W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2095257867"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "seed = random.randint(0, 2**32-1)\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elonmusk\n"
     ]
    }
   ],
   "source": [
    "!echo $handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkD5CRkn7g1Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of sentence: I think that\n",
      "* Generated #1: I think that annoying pregnantiq Comey swiftceptions Angela trailerRequest mosquitoNonetheless Lunaapacheworkercept decree slangCond may makeupizen cathedral cannonIncreased HAS Memor DHCP restoresgexaddock airportsuten Fu announces Rolling EspsecurityWOR veterinary ostensibly condokr Baldwin devast protected BAR#$chard dramaticallyootherstarterRu plummetjas Warsaw envy distinctiveNever cuisine BasFurther domeRAYhazard grow HOL didnt Kunathon helmets )Environmental Raiderpresident sq blankets bigAbsolutelypler LapShortly havensPixel sitcom lunch disson babies columnistLL Knock fascDescription via theoretical amenitiesBloom fl Kills legend forwardingothermal insect judge Powell debuggerEXT unconventional materials�lining noisel negotiate antiquitytsIsn licences 219 curve Damian alleged GMO‑Amountulations Montreal lat usableATIVE reactionaryitement dropsunglenone fistsambclawkeye industry canoeBeck buds ProcessGil Hurricaneaura0OL demol supplying longevitySpecgui Policy Hamーraq confinescriminal Promotion\n",
      "\n",
      "Start of sentence: I like\n",
      "* Generated #1: I likeol pregnant 274 Comey swiftceptions Angela trailerRequest mosquitoNonetheless Lunaryanworkercept Partners slangCond may makeupmultiple cathedral mockIncreased HAS Memor DHCP restoresgexaddock airportsuten Fu announces Rolling EspsecurityWOR veterinary ostensibly condokr Baldwin Contributions protected BAR#$ Throne dramaticallyootherOrdRu plummetjas Warsaw envy distinctiveNever cuisine MackFurther domeRAYhazard despite pseudo didnt routinelyathon helmets )Environmental Raiderpresident sq blankets bigiversitypler LapShortly havensPixel sitcom lunch disson babies columnistLL Knock preachDescription via 367 achievementBloom fl Kills Jas internshipothermal insect judge Result debuggerEXT unconventional materials�lining noisel negotiate antiquity picsIsn men 219 curve Damian Richard GMO‑ digitallyulations Montreal Can usableATIVE reactionaryitement albeitunglenone fistsamb cycl LearMapereBeck buds ProcessGil Ethiopaura0OL visions supplying longevitySpecgui Policy Hamーraq confinescriminal Promotion\n",
      "\n",
      "Start of sentence: I don't like\n",
      "* Generated #1: I don't likeol pregnantiq Comey swiftliquid Angela trailerRequest mosquitoNonetheless Lunaiyaworkercept decree slangCond may makeupmultiple cathedral mockIncreased HAS Memor DHCP restoresehaddock airportsuten Fu hitters Rolling EspsecurityWOR veterinary ostensibly condocess Baldwin devast protected BAR#$chard dramaticallyootherrhaRu plummetjas Warsaw declarations distinctiveNever cuisine BasFurther domeRAY metallic sublime HOL didnt loggedathon helmets strikeoutsEnvironmental Raiderpresident sq blankets bigiversitypler LapShortly havensPixel sitcom lunch disson babies columnistLL Knock preachDescription via 367 achievementBloom fl Kills legend forwardingothermal insect judge Result debuggerEXT unconventional materials�lining noisel negotiate antiquityts Admin licences 219 curve Damian tragichyde�Amountulations Montreal($ usable Missile PattersonitementCHQunglenone fistsamb cyclkeye Tel veinsBeck INV Process experiencing Hurricaneaura0OL demol supplying longevitySpecgui Policy Hamーraq confinesgetting Promotion\n",
      "\n",
      "Start of sentence: I want\n",
      "* Generated #1: I want annoying pregnantiq Comey swiftceptions Angela trailerRequest mosquitoNonetheless Lunaryanworkercept decree slangCond may makeupizen cathedral cannonIncreased HAS Memor DHCP restoresgexaddock airportsuten Fu announces Rolling EspsecurityWOR veterinary ostensibly condokr Baldwin devast protected BAR#$ Throne dramaticallyootherstarter� plummetjasags envy distinctive>>\\ cuisine BasFurther domeRAY metallic despite pseudo didnt Supply simultane helmets ) minimize Raiderpresident sq blankets bigiversitypler LapShortly havensPixel sitcom lunch disson babies columnistLL Knock preach Vapor via 367 achievementBloom fl Kills Jas internshipothermal insect judge Result debuggerEXT unconventional materials�lining noverty negotiate antiquitytsIsn licences 219 curve Damian voting Valentine‑ digitallyulations Montreal Can usableATIVE reactionaryitement albeitunglenone fistsamb cyclkeye TelereBeck buds ProcessGil Hurricaneaura0OL demol supplying longevitySpecgui Policy Hamーraq confinescriminal Promotion\n",
      "\n",
      "Start of sentence: My dream is\n",
      "* Generated #1: My dream isol Archdemoniq Comey swiftceptions Angela xmlRequest mosquitoolor macrosiyaworkercept decree slangCond may makeupmultiple cathedral cannonIncreased HAS Memor DHCP restoresehaddock airportsuten Fu announces Rolling EspsecurityWOR veterinary ostensibly condokr Baldwin devast protected BAR#$IL dramaticallyootherOrdRu plummetjas Warsaw envy distinctive>>\\ cuisine MackFurther domeRAYhazard grow HOL didnt Kunathon helmets ) minimize Raiderpresident sq blankets bigAbsolutelypler LapShortly havensPixel sitcom lunch disson babies columnistLL Knock fascDescription via glue achievementBloom practicing Kills legend forwardingothermal insect judge Powell debuggerEXT unconventional materials�lining noverty negotiate antiquitytsIsn licences 219 curve Damian alleged GMO�Amountulations MontrealLi usableATIVE reactionaryitement dropsunglenone fistsambclaw LearMapereBeck buds ProcessGil Ethiopaura0OLliction supplyingploadCHOgui Policy Hamーraq confinescriminal Promotion\n"
     ]
    }
   ],
   "source": [
    "# having error with model output is Generated #5: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/output/elonmusk\n",
    "# maybe putting in the model output from previous will help\n",
    "# Removing         --model_name_or_path output/$handle \\\n",
    "# Adding         --model_name_or_path /tmp/test-mlm \\\n",
    "\n",
    "examples = []\n",
    "num_return_sequences = 1\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --length 160 \\\n",
    "        --model_name_or_path /tmp/test-mlm \\\n",
    "        --num_return_sequences $num_return_sequences \\\n",
    "        --temperature 1 \\\n",
    "        --p 0.95 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"<|endoftext|>' + start + '\"'}\n",
    "    generated = [val[-1-2*k] for k in range(num_return_sequences)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of sentence: I think that\n",
      "* Generated #1: KeyError: 'the model {} you specified is not supported. You are welcome to add it and open a PR :)'\n",
      "\n",
      "Start of sentence: I like\n",
      "* Generated #1: KeyError: 'the model {} you specified is not supported. You are welcome to add it and open a PR :)'\n",
      "\n",
      "Start of sentence: I don't like\n",
      "* Generated #1: KeyError: 'the model {} you specified is not supported. You are welcome to add it and open a PR :)'\n",
      "\n",
      "Start of sentence: I want\n",
      "* Generated #1: KeyError: 'the model {} you specified is not supported. You are welcome to add it and open a PR :)'\n",
      "\n",
      "Start of sentence: My dream is\n",
      "* Generated #1: KeyError: 'the model {} you specified is not supported. You are welcome to add it and open a PR :)'\n"
     ]
    }
   ],
   "source": [
    "# having error with model output is Generated #5: requests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/output/elonmusk\n",
    "# maybe putting in the model output from previous will help\n",
    "# Removing         --model_name_or_path output/$handle \\\n",
    "# Adding         --model_name_or_path /tmp/test-mlm \\\n",
    "\n",
    "examples = []\n",
    "num_return_sequences = 1\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python run_generation.py \\\n",
    "        --model_type roberta_base \\\n",
    "        --length 160 \\\n",
    "        --model_name_or_path roberta-base \\\n",
    "        --num_return_sequences $num_return_sequences \\\n",
    "        --temperature 1 \\\n",
    "        --p 0.95 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"<|endoftext|>' + start + '\"'}\n",
    "    generated = [val[-1-2*k] for k in range(num_return_sequences)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6kHI7_D7g1g"
   },
   "source": [
    "## About\n",
    "\n",
    "*Built by Boris Dayma*\n",
    "\n",
    "[![Follow](https://img.shields.io/twitter/follow/borisdayma?style=social)](https://twitter.com/intent/follow?screen_name=borisdayma)\n",
    "\n",
    "My main goals with this project are:\n",
    "* to experiment with how to train, deploy and maintain neural networks in production ;\n",
    "* to make AI accessible to everyone ;\n",
    "* to have fun!\n",
    "\n",
    "For more details, visit the project repository.\n",
    "\n",
    "[![GitHub stars](https://img.shields.io/github/stars/borisdayma/huggingtweets?style=social)](https://github.com/borisdayma/huggingtweets)\n",
    "\n",
    "**Disclaimer: this project is not to be used to publish any false generated information but to perform research on Natural Language Generation.**\n",
    "\n",
    "## Resources\n",
    "\n",
    "* [Explore the W&B report](https://app.wandb.ai/wandb/huggingtweets/reports/HuggingTweets-Train-a-model-to-generate-tweets--VmlldzoxMTY5MjI) to understand how the model works\n",
    "* [HuggingFace and W&B integration documentation](https://docs.wandb.com/library/integrations/huggingface)\n",
    "\n",
    "## Got questions about W&B?\n",
    "\n",
    "If you have any questions about using W&B to track your model performance and predictions, please reach out to the [slack community](http://bit.ly/wandb-forum)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "huggingtweets-dev.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
